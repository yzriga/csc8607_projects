dataset:
  name: tiny-imagenet
  root: "./data"
  split:
    train: 100000
    val: 10000
    test: 10000      # notre split artificiel val/test
  download: true
  num_workers: 4
  shuffle: true

preprocess:
  resize: [64, 64]
  normalize:
    mean: [0.485, 0.456, 0.406]
    std:  [0.229, 0.224, 0.225]

augment:
  random_flip: true
  random_crop: null
  color_jitter: null

model:
  type: preact_resnet
  num_classes: 200
  input_shape: [3, 64, 64]

  # architecture du modèle
  blocks: [2, 2, 2]     # B1, B2, B3
  channels: [64, 128, 256]   # C1, C2, C3

  activation: relu
  dropout: 0.0
  batch_norm: true
  residual: true

train:
  seed: 42
  device: auto              # "cpu", "cuda", ou "auto"
  batch_size: 64
  epochs: 30
  max_steps: null           # entier ou null
  overfit_small: false      # true pour sur-apprendre sur un petit échantillon

  optimizer:
    name: adam              # sgd/adam/rmsprop
    lr: 0.001
    weight_decay: 0.0005
    momentum: 0.9           # utile si SGD

  scheduler:
    name: none              # none/step/cosine/onecycle
    step_size: 10
    gamma: 0.1
    warmup_steps: 0

metrics:
  classification:           # ex: ["accuracy", "f1"]
    - accuracy
  regression: []            # ex: ["mae", "rmse"]

hparams:                    # espace pour mini grid search
  lr: [0.0005, 0.001, 0.005]
  batch_size: [32, 64]
  weight_decay: [0.0, 0.0005]

paths:
  runs_dir: "./runs"
  artifacts_dir: "./artifacts"
